{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b1949c",
   "metadata": {},
   "source": [
    "# Group Modeling Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615349d",
   "metadata": {},
   "source": [
    "## Created by: Kim Buesser, Griffin Brown, and Kathryn O'Connor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f91dc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Home Credit is an international finance provider aiming to increase financial inclusion by providing financial loans to those with little to no credit history. Rather than rely on credit history, Home Credit uses a variety of data to assess an individual’s repayment abilities.\n",
    "\n",
    "While they currently use a variety of statistical and machine learning methods to make their predictions, Home Credit is hoping to further harness their data to ensure that those who are capable of repaying loans are not rejected. Achieving greater accuracy on whether an individual is capable of loan repayment will not only allow Home Credit to broaden their customer base (and in turn, profit) they will also be able to help a larger population of individuals who are in need of loans.\n",
    "\n",
    "In this notebook, different models are explored with the aim of developing a model that beats the majority class classifier and can be used to help solve their business problem.\n",
    "\n",
    "Processes such cross-validation and feature engineering are used with different models to help optimize overall performance, and metrics will be used to compare to the majority class classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2589c6",
   "metadata": {},
   "source": [
    "## Our Philosophy\n",
    "\n",
    "In our project journey, we adopted a collaborative mindset. We agreed that keeping our data manipulations and dataset usage consistent across all models would streamline our process and maintain coherence. This unified approach not only enhances efficiency but also cultivates a harmonious environment conducive to reaching our mutual goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae943774",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d634a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aaf4a7",
   "metadata": {},
   "source": [
    "## File Upload into Environment\n",
    "\n",
    "Here, we stored our files in a Google Cloud Storage so we would not have to read directly from our local machines and change filepaths during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "839bec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/0tw9tkvj2gx_v0qrnty9rt0r0000gn/T/ipykernel_30847/3497087744.py:14: DtypeWarning: Columns (90) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_url)\n"
     ]
    }
   ],
   "source": [
    "# Public URL after making the file public in the format 'https://storage.googleapis.com/...'\n",
    "file_url = 'https://storage.googleapis.com/home_credit_files/application_train.csv'\n",
    "test_url = 'https://storage.googleapis.com/home_credit_files/application_test.csv'\n",
    "# POS_CASH_balance_url = 'https://storage.googleapis.com/home_credit_files/POS_CASH_balance.csv'\n",
    "# bureau_url = 'https://storage.googleapis.com/home_credit_files/bureau.csv'\n",
    "# bureau_balance_url = 'https://storage.googleapis.com/home_credit_files/bureau.csv'\n",
    "# credit_card_balance = 'https://storage.googleapis.com/home_credit_files/credit_card_balance.csv'\n",
    "# installments_payments = 'https://storage.googleapis.com/home_credit_files/credit_card_balance.csv'\n",
    "# previous_application = 'https://storage.googleapis.com/home_credit_files/previous_application.csv'\n",
    "sample_sub = 'https://storage.googleapis.com/home_credit_files/sample_submission.csv'\n",
    "\n",
    "\n",
    "# Read the CSV directly from the URL\n",
    "df = pd.read_csv(file_url)\n",
    "test_df = pd.read_csv(test_url)\n",
    "\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c84ee0",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Below, we perform EDA on the train data set to prepare it for modeling. We began by analyzing the initial data set and obtaining the proportion of the target variable in the train set. About 92% of the data is 0, indicating those that repayed their loans on time. About 8% represent 1, those that did not repay on time.\n",
    "\n",
    "Next, we explored missing variables. We calculated the percentage of missing values per column, and after analyzing the descriptions for each, decided to remove those that were missing greater than 10% of their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7734857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 122)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d92972e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET\n",
       "0    282686\n",
       "1     24825\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c29d837e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET\n",
       "0    91.927118\n",
       "1     8.072882\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TARGET'].value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae7df1",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65bc9d0",
   "metadata": {},
   "source": [
    "### Evaluating Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad44bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data set\n",
    "\n",
    "# calculate total number of missing values for each column\n",
    "missing_values_train = df.isnull().sum()\n",
    "\n",
    "# calculate total number of rows\n",
    "total_rows_train = df.shape[0]\n",
    "\n",
    "# calculate percentage of missing values for each column\n",
    "pct_missing_train = (missing_values_train / total_rows_train) * 100\n",
    "\n",
    "# sort output\n",
    "pct_missing_sorted_train = pct_missing_train.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec216893",
   "metadata": {},
   "outputs": [],
   "source": [
    "missings = pct_missing_sorted_train.to_frame(name='MissingPercentage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4d91c",
   "metadata": {},
   "source": [
    "### Removing columns with greater than 10% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3431d878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_missings = missings[(missings['MissingPercentage'] < 10)]\n",
    "filter_missings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5d19f",
   "metadata": {},
   "source": [
    "### Removing 'FLAG' columns and ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e33e4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(filter_missings.index)\n",
    "column_list = [item for item in columns if 'FLAG' not in item]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabcb8ab",
   "metadata": {},
   "source": [
    "### Removing additional unhelpful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33bc77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = column_list\n",
    "list2 = ['NAME_TYPE_SUITE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "\n",
    "# Initialize an empty list to store items from list1 that are not in list2\n",
    "items_not_in_list2 = []\n",
    "\n",
    "# Iterate through each item in list1\n",
    "for item in list1:\n",
    "    # Check if the item is not in list2\n",
    "    if item not in list2:\n",
    "        # Add the item to the items_not_in_list2 list\n",
    "        items_not_in_list2.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93bf9518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items_not_in_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "53e7bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "selected_df = df[items_not_in_list2]\n",
    "selected_df\n",
    "\n",
    "test_filtered_columns = [item for item in items_not_in_list2 if item != 'TARGET']\n",
    "test_selected_df = test_df[test_filtered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "38db0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'TARGET' column\n",
    "target = selected_df['TARGET']\n",
    "\n",
    "# Remove the 'TARGET' column from the dataframe and reassign the result back to selected_df\n",
    "selected_df = selected_df.drop(columns=['TARGET'])\n",
    "\n",
    "# Insert the 'TARGET' column at the beginning of the dataframe\n",
    "selected_df.insert(0, 'TARGET', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c523e",
   "metadata": {},
   "source": [
    "### Fix problematic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8df3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.loc[:, 'DAYS_EMPLOYED'] = selected_df['DAYS_EMPLOYED'].replace(365243, 0)\n",
    "selected_df = selected_df[selected_df['AMT_INCOME_TOTAL'] <= 9000000]\n",
    "\n",
    "test_selected_df.loc[:, 'DAYS_EMPLOYED'] = test_selected_df['DAYS_EMPLOYED'].replace(365243, 0)\n",
    "test_selected_df = test_selected_df[test_selected_df['AMT_INCOME_TOTAL'] <= 9000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b53c84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307508, 32)\n",
      "(48744, 31)\n"
     ]
    }
   ],
   "source": [
    "print(selected_df.shape)\n",
    "print(test_selected_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8de43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = selected_df.drop(columns = ['HOUR_APPR_PROCESS_START', 'DAYS_ID_PUBLISH'])\n",
    "test_selected_df = test_selected_df.drop(columns = ['HOUR_APPR_PROCESS_START', 'DAYS_ID_PUBLISH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37eb382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_selected_df)-set(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b449bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abs value of negatives\n",
    "selected_df['DAYS_LAST_PHONE_CHANGE'] = selected_df.loc[:, 'DAYS_LAST_PHONE_CHANGE'].abs()\n",
    "selected_df['DAYS_REGISTRATION'] = selected_df.loc[:, 'DAYS_REGISTRATION'].abs()\n",
    "selected_df['DAYS_EMPLOYED'] = selected_df.loc[:, 'DAYS_EMPLOYED'].abs()\n",
    "selected_df['DAYS_BIRTH'] = selected_df.loc[:, 'DAYS_BIRTH'].abs()\n",
    "\n",
    "test_selected_df['DAYS_LAST_PHONE_CHANGE'] = test_selected_df.loc[:, 'DAYS_LAST_PHONE_CHANGE'].abs()\n",
    "test_selected_df['DAYS_REGISTRATION'] = test_selected_df.loc[:, 'DAYS_REGISTRATION'].abs()\n",
    "test_selected_df['DAYS_EMPLOYED'] = test_selected_df.loc[:, 'DAYS_EMPLOYED'].abs()\n",
    "test_selected_df['DAYS_BIRTH'] = test_selected_df.loc[:, 'DAYS_BIRTH'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a4096",
   "metadata": {},
   "source": [
    "## Data Preparation Summary\n",
    "\n",
    "Our initial approach was fairly simplistic for our first attempt. We started by dropped columns that had too much missing data with a cutoff of 10%. We then cut out all 'FLAG' columns as we thought these weren't needed for our basic approach. We fixed a couple columns that had some bad information, and all the while we did the same manipulations to the test set as we did with the train set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4c07c",
   "metadata": {},
   "source": [
    "## Preparing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e6fac2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.789654</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>20560.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>0</td>\n",
       "      <td>568800.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5170.0</td>\n",
       "      <td>2329</td>\n",
       "      <td>19241</td>\n",
       "      <td>0.018850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.291656</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>17370.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>222768.0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9118.0</td>\n",
       "      <td>4469</td>\n",
       "      <td>18064</td>\n",
       "      <td>0.035792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.699787</td>\n",
       "      <td>630000.0</td>\n",
       "      <td>69777.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>0</td>\n",
       "      <td>663264.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2175.0</td>\n",
       "      <td>4458</td>\n",
       "      <td>20038</td>\n",
       "      <td>0.019101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.509677</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>49018.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1805.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1866</td>\n",
       "      <td>13976</td>\n",
       "      <td>0.026392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.425687</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>32067.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>1</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>2191</td>\n",
       "      <td>13040</td>\n",
       "      <td>0.010032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48739</th>\n",
       "      <td>0.648575</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>17473.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>0</td>\n",
       "      <td>412560.0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9094.0</td>\n",
       "      <td>5169</td>\n",
       "      <td>19970</td>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48740</th>\n",
       "      <td>0.684596</td>\n",
       "      <td>495000.0</td>\n",
       "      <td>31909.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>622413.0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3015.0</td>\n",
       "      <td>1149</td>\n",
       "      <td>11186</td>\n",
       "      <td>0.035792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48741</th>\n",
       "      <td>0.632770</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>33205.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>1</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2681.0</td>\n",
       "      <td>3037</td>\n",
       "      <td>15922</td>\n",
       "      <td>0.026392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48742</th>\n",
       "      <td>0.445701</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>25128.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2308.0</td>\n",
       "      <td>0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1461.0</td>\n",
       "      <td>2731</td>\n",
       "      <td>13968</td>\n",
       "      <td>0.018850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48743</th>\n",
       "      <td>0.456541</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>24709.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>0</td>\n",
       "      <td>312768.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>633</td>\n",
       "      <td>13962</td>\n",
       "      <td>0.006629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48744 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       EXT_SOURCE_2  AMT_GOODS_PRICE  AMT_ANNUITY  CNT_FAM_MEMBERS  \\\n",
       "0          0.789654         450000.0      20560.5              2.0   \n",
       "1          0.291656         180000.0      17370.0              2.0   \n",
       "2          0.699787         630000.0      69777.0              2.0   \n",
       "3          0.509677        1575000.0      49018.5              4.0   \n",
       "4          0.425687         625500.0      32067.0              3.0   \n",
       "...             ...              ...          ...              ...   \n",
       "48739      0.648575         270000.0      17473.5              1.0   \n",
       "48740      0.684596         495000.0      31909.5              4.0   \n",
       "48741      0.632770         315000.0      33205.5              3.0   \n",
       "48742      0.445701         450000.0      25128.0              2.0   \n",
       "48743      0.456541         270000.0      24709.5              2.0   \n",
       "\n",
       "       DAYS_LAST_PHONE_CHANGE  CNT_CHILDREN  AMT_CREDIT  AMT_INCOME_TOTAL  \\\n",
       "0                      1740.0             0    568800.0          135000.0   \n",
       "1                         0.0             0    222768.0           99000.0   \n",
       "2                       856.0             0    663264.0          202500.0   \n",
       "3                      1805.0             2   1575000.0          315000.0   \n",
       "4                       821.0             1    625500.0          180000.0   \n",
       "...                       ...           ...         ...               ...   \n",
       "48739                   684.0             0    412560.0          121500.0   \n",
       "48740                     0.0             2    622413.0          157500.0   \n",
       "48741                   838.0             1    315000.0          202500.0   \n",
       "48742                  2308.0             0    450000.0          225000.0   \n",
       "48743                   327.0             0    312768.0          135000.0   \n",
       "\n",
       "       LIVE_CITY_NOT_WORK_CITY  REG_CITY_NOT_WORK_CITY  \\\n",
       "0                            0                       0   \n",
       "1                            0                       0   \n",
       "2                            0                       0   \n",
       "3                            0                       0   \n",
       "4                            1                       1   \n",
       "...                        ...                     ...   \n",
       "48739                        0                       0   \n",
       "48740                        1                       1   \n",
       "48741                        0                       0   \n",
       "48742                        1                       1   \n",
       "48743                        0                       0   \n",
       "\n",
       "       REG_CITY_NOT_LIVE_CITY  LIVE_REGION_NOT_WORK_REGION  \\\n",
       "0                           0                            0   \n",
       "1                           0                            0   \n",
       "2                           0                            0   \n",
       "3                           0                            0   \n",
       "4                           0                            0   \n",
       "...                       ...                          ...   \n",
       "48739                       0                            0   \n",
       "48740                       0                            0   \n",
       "48741                       0                            0   \n",
       "48742                       0                            1   \n",
       "48743                       0                            0   \n",
       "\n",
       "       REG_REGION_NOT_WORK_REGION  REG_REGION_NOT_LIVE_REGION  \\\n",
       "0                               0                           0   \n",
       "1                               0                           0   \n",
       "2                               0                           0   \n",
       "3                               0                           0   \n",
       "4                               0                           0   \n",
       "...                           ...                         ...   \n",
       "48739                           0                           0   \n",
       "48740                           0                           0   \n",
       "48741                           0                           0   \n",
       "48742                           1                           0   \n",
       "48743                           0                           0   \n",
       "\n",
       "       REGION_RATING_CLIENT_W_CITY  REGION_RATING_CLIENT  DAYS_REGISTRATION  \\\n",
       "0                                2                     2             5170.0   \n",
       "1                                2                     2             9118.0   \n",
       "2                                2                     2             2175.0   \n",
       "3                                2                     2             2000.0   \n",
       "4                                2                     2             4000.0   \n",
       "...                            ...                   ...                ...   \n",
       "48739                            3                     3             9094.0   \n",
       "48740                            2                     2             3015.0   \n",
       "48741                            2                     2             2681.0   \n",
       "48742                            2                     2             1461.0   \n",
       "48743                            2                     2             1072.0   \n",
       "\n",
       "       DAYS_EMPLOYED  DAYS_BIRTH  REGION_POPULATION_RELATIVE  \n",
       "0               2329       19241                    0.018850  \n",
       "1               4469       18064                    0.035792  \n",
       "2               4458       20038                    0.019101  \n",
       "3               1866       13976                    0.026392  \n",
       "4               2191       13040                    0.010032  \n",
       "...              ...         ...                         ...  \n",
       "48739           5169       19970                    0.002042  \n",
       "48740           1149       11186                    0.035792  \n",
       "48741           3037       15922                    0.026392  \n",
       "48742           2731       13968                    0.018850  \n",
       "48743            633       13962                    0.006629  \n",
       "\n",
       "[48744 rows x 20 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab numeric columns\n",
    "numeric_cols = selected_df.drop(['SK_ID_CURR'], axis=1).select_dtypes(include='number')\n",
    "numeric_cols\n",
    "\n",
    "test_numeric_cols = test_selected_df.drop(['SK_ID_CURR'], axis=1).select_dtypes(include='number')\n",
    "test_numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f4beb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EXT_SOURCE_2', 'AMT_GOODS_PRICE', 'AMT_ANNUITY', 'CNT_FAM_MEMBERS', 'DAYS_LAST_PHONE_CHANGE', 'CNT_CHILDREN', 'AMT_CREDIT', 'AMT_INCOME_TOTAL', 'LIVE_CITY_NOT_WORK_CITY', 'REG_CITY_NOT_WORK_CITY', 'REG_CITY_NOT_LIVE_CITY', 'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION', 'REGION_RATING_CLIENT_W_CITY', 'REGION_RATING_CLIENT', 'DAYS_REGISTRATION', 'DAYS_EMPLOYED', 'DAYS_BIRTH', 'REGION_POPULATION_RELATIVE']\n",
      "['EXT_SOURCE_2', 'AMT_GOODS_PRICE', 'AMT_ANNUITY', 'CNT_FAM_MEMBERS', 'DAYS_LAST_PHONE_CHANGE', 'CNT_CHILDREN', 'AMT_CREDIT', 'AMT_INCOME_TOTAL', 'LIVE_CITY_NOT_WORK_CITY', 'REG_CITY_NOT_WORK_CITY', 'REG_CITY_NOT_LIVE_CITY', 'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION', 'REGION_RATING_CLIENT_W_CITY', 'REGION_RATING_CLIENT', 'DAYS_REGISTRATION', 'DAYS_EMPLOYED', 'DAYS_BIRTH', 'REGION_POPULATION_RELATIVE']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of numeric column names excluding target for imputations\n",
    "\n",
    "column_names = numeric_cols.columns.tolist()\n",
    "my_list = [x for x in column_names if x != 'TARGET']\n",
    "print(my_list)\n",
    "\n",
    "test_column_names = numeric_cols.columns.tolist()\n",
    "test_my_list = [x for x in column_names if x != 'TARGET']\n",
    "print(test_my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8037c13",
   "metadata": {},
   "source": [
    "### N/a Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0598584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleImputer instance\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Fit and transform the selected numeric columns\n",
    "selected_df.loc[:, my_list] = imputer.fit_transform(selected_df.loc[:, my_list])\n",
    "\n",
    "test_selected_df.loc[:, test_my_list] = imputer.fit_transform(test_selected_df.loc[:, test_my_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bb82c",
   "metadata": {},
   "source": [
    "## Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee0816ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df['CREDIT_TO_INCOME'] = selected_df['AMT_CREDIT']/selected_df['AMT_INCOME_TOTAL']\n",
    "\n",
    "test_selected_df['CREDIT_TO_INCOME'] = test_selected_df['AMT_CREDIT']/test_selected_df['AMT_INCOME_TOTAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d6104",
   "metadata": {},
   "source": [
    "### Standard Scaler to scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a121b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalar_list = ['EXT_SOURCE_2','AMT_GOODS_PRICE','AMT_ANNUITY','AMT_CREDIT','AMT_INCOME_TOTAL','REGION_RATING_CLIENT_W_CITY','REGION_RATING_CLIENT','CREDIT_TO_INCOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "817f6e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307508, 31)\n",
      "(48744, 30)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "selected_df[my_list] = scaler.fit_transform(selected_df[my_list])\n",
    "\n",
    "test_selected_df[test_my_list] = scaler.fit_transform(test_selected_df[test_my_list])\n",
    "\n",
    "print(selected_df.shape)\n",
    "print(test_selected_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c701a",
   "metadata": {},
   "source": [
    "### Dummy Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "838f390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307508, 110)\n",
      "(48744, 106)\n",
      "set()\n",
      "{'TARGET', 'CODE_GENDER_XNA', 'NAME_FAMILY_STATUS_Unknown', 'NAME_INCOME_TYPE_Maternity leave'}\n",
      "(307508, 107)\n",
      "(48744, 106)\n"
     ]
    }
   ],
   "source": [
    "# dummy encoding data set\n",
    "\n",
    "selected_df = pd.get_dummies(selected_df, drop_first=True)\n",
    "\n",
    "test_selected_df = pd.get_dummies(test_selected_df, drop_first=True)\n",
    "\n",
    "print(selected_df.shape)\n",
    "print(test_selected_df.shape)\n",
    "\n",
    "print(set(test_selected_df)-set(selected_df))\n",
    "print(set(selected_df)-set(test_selected_df))\n",
    "\n",
    "columns_to_drop = set(selected_df.columns) - set(test_selected_df.columns)\n",
    "columns_to_drop.discard('TARGET')\n",
    "selected_df = selected_df.drop(columns=list(columns_to_drop), axis=1)\n",
    "\n",
    "print(selected_df.shape)\n",
    "print(test_selected_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a52f59",
   "metadata": {},
   "source": [
    "### Splitting Target and Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fa69c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = selected_df.drop(columns=['TARGET', 'SK_ID_CURR'])\n",
    "y = selected_df['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82fdf6",
   "metadata": {},
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b58b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = selected_df.drop('TARGET', axis=1)\n",
    "y = selected_df['TARGET']\n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority = data[data.TARGET==0]\n",
    "minority = data[data.TARGET==1]\n",
    "\n",
    "# Upsample minority class\n",
    "minority_upsampled = resample(minority, \n",
    "                              replace=True,     # sample with replacement\n",
    "                              n_samples=len(majority),    # to match majority class size\n",
    "                              random_state=42)  # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "upsampled_data = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "# Checking the new class distribution\n",
    "upsampled_data.TARGET.value_counts()\n",
    "\n",
    "X_upsampled = upsampled_data.drop('TARGET', axis=1)\n",
    "y_upsampled = upsampled_data['TARGET']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6a567",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "393953e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9192736449133031\n",
      "Train R2 =  0.9192736449133031\n"
     ]
    }
   ],
   "source": [
    "# Initializing and training the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter = 1000)\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "# Making predictions on the validation set\n",
    "y_pred = log_reg.predict(X)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Train Accuracy:\", accuracy)\n",
    "print(\"Train R2 = \", log_reg.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9d8b469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score (Original): 0.5022168461434817\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for the positive class (class 1) using the trained model\n",
    "y_prob = log_reg.predict_proba(X)[:, 1]\n",
    "\n",
    "# Calculate false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = auc(fpr, tpr)\n",
    "print(\"AUC Score (Original):\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597c3be",
   "metadata": {},
   "source": [
    "## Logistic Regression Upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "799f5dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (Upsampled): 0.5\n",
      "Train R2 (Upsampled): 0.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Upsample the dataset\n",
    "X_upsampled, y_upsampled = smote.fit_resample(X_upsampled, y_upsampled)\n",
    "\n",
    "# Initialize and train the logistic regression model with the upsampled dataset\n",
    "log_reg_upsampled = LogisticRegression(max_iter=2000)\n",
    "log_reg_upsampled.fit(X_upsampled, y_upsampled)\n",
    "\n",
    "# Make predictions on the upsampled dataset\n",
    "y_pred_upsampled = log_reg_upsampled.predict(X_upsampled)\n",
    "\n",
    "# Calculate accuracy on the upsampled dataset\n",
    "accuracy_upsampled = accuracy_score(y_upsampled, y_pred_upsampled)\n",
    "print(\"Train Accuracy (Upsampled):\", accuracy_upsampled)\n",
    "print(\"Train R2 (Upsampled):\", log_reg_upsampled.score(X_upsampled, y_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a670c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score (Upsampled): 0.5032005156743218\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for the positive class (class 1) using upsampled model\n",
    "y_prob_upsampled = log_reg_upsampled.predict_proba(X_upsampled)[:, 1]\n",
    "\n",
    "# Calculate false positive rate, true positive rate, and thresholds for upsampled model\n",
    "fpr_upsampled, tpr_upsampled, thresholds_upsampled = roc_curve(y_upsampled, y_prob_upsampled)\n",
    "\n",
    "# Calculate AUC score for upsampled model\n",
    "auc_score_upsampled = auc(fpr_upsampled, tpr_upsampled)\n",
    "print(\"AUC Score (Upsampled):\", auc_score_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e72719",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf733c",
   "metadata": {},
   "source": [
    "### Mean Squared Error - simple logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c5764341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Call the function  and define a scoring function - set it as \"neg_mean_squared_error\"\n",
    "log_scores = cross_val_score(log_reg, \n",
    "                             X, \n",
    "                             y,\n",
    "                             scoring=\"neg_mean_squared_error\",   # higher return values are better for this function\n",
    "                             cv=10                               # create 10 folds\n",
    ")\n",
    "\n",
    "# to obtain RMSE, we need to negate the scores obtained  from cross_val_score and take the sqrt.\n",
    "# this returns an array of 10 values\n",
    "log_rmse_scores = np.sqrt(-log_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cc6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8014dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_scores(log_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95cb05",
   "metadata": {},
   "source": [
    "### Mean Squared Error - upsampled logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scores_upsampled = cross_val_score(log_reg_upsampled, \n",
    "                                       X_upsampled, \n",
    "                                       y_upsampled,\n",
    "                                       scoring=\"neg_mean_squared_error\",   # higher return values are better for this function\n",
    "                                       cv=10 # create 10 folds\n",
    ")\n",
    "\n",
    "log_rmse_scores_upsampled = np.sqrt(-log_scores_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "43b475a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.6295993  0.70710053 0.70710053 0.70711303 0.70711303 0.54084786\n",
      " 0.50124983 0.70711303 0.70710678 0.70710678]\n",
      "Mean: 0.6621450712088042\n",
      "Standard deviation: 0.0747067396895937\n"
     ]
    }
   ],
   "source": [
    "display_scores(log_rmse_scores_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca95dc",
   "metadata": {},
   "source": [
    "### R Squared - simple logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a0db95d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [-0.08779936 -0.08779936 -0.08779936 -0.08779936 -0.08783784 -0.08783784\n",
      " -0.08783784 -0.08783784 -0.08780246 -0.08780246]\n",
      "Mean: -0.08781537003882575\n",
      "Standard deviation: 1.837990557490588e-05\n"
     ]
    }
   ],
   "source": [
    "log_r2_scores = cross_val_score(log_reg, \n",
    "                             X, y,\n",
    "                             scoring=\"r2\", \n",
    "                             cv=10)\n",
    "display_scores(log_r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b2d0d463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.55891164 0.47320167 0.77771346 0.84179435 0.23103222 0.68945731\n",
      " 0.5061176  0.51077052 0.49801962 0.47524805]\n",
      "Mean: 0.5562266429015005\n",
      "Standard deviation: 0.1659910446997831\n"
     ]
    }
   ],
   "source": [
    "log_auc_scores = cross_val_score(log_reg, \n",
    "                             X, \n",
    "                             y,\n",
    "                             scoring=\"roc_auc\", \n",
    "                             cv=10                               \n",
    ")\n",
    "log_mean_auc = np.mean(log_auc_scores)\n",
    "display_scores(log_auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce8ae9",
   "metadata": {},
   "source": [
    "### R squared - upsampled logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "27459e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [-1.00003538 -0.99996463 -0.99996463 -1.00003538 -1.00003538 -1.00003538\n",
      " -0.99996463 -1.00003538 -1.         -1.        ]\n",
      "Mean: -1.000007075513384\n",
      "Standard deviation: 3.0839294641349914e-05\n"
     ]
    }
   ],
   "source": [
    "log_up_r2_scores = cross_val_score(log_reg_upsampled, \n",
    "                             X_upsampled, \n",
    "                             y_upsampled,\n",
    "                             scoring=\"r2\", \n",
    "                             cv=10)\n",
    "display_scores(log_up_r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e83d869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.39590006 0.37154273 0.26070667 0.10165558 0.47117858 0.50665821\n",
      " 0.39784145 0.36870688 0.30433986 0.25336769]\n",
      "Mean: 0.3431897719134749\n",
      "Standard deviation: 0.11182254153778533\n"
     ]
    }
   ],
   "source": [
    "log_up_auc_scores = cross_val_score(log_reg_upsampled, \n",
    "                             X_upsampled, \n",
    "                             y_upsampled,\n",
    "                             scoring=\"roc_auc\", \n",
    "                             cv=10                               \n",
    ")\n",
    "log_up_mean_auc = np.mean(log_up_auc_scores)\n",
    "display_scores(log_up_auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15926070",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9a2589f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kathrynoconnor/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9906604055829442\n",
      "Train Accuracy using built-in score: 0.9906604055829442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, \n",
    "                        eval_metric='logloss', \n",
    "                        n_estimators=100, \n",
    "                        max_depth=15\n",
    "                        )\n",
    "\n",
    "xgb_clf.fit(X, y)\n",
    "\n",
    "xgb_y_pred = xgb_clf.predict(X)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y, xgb_y_pred)\n",
    "\n",
    "print(\"Train Accuracy:\", xgb_accuracy)\n",
    "print(\"Train Accuracy using built-in score:\", xgb_clf.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3648bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kathrynoconnor/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/Users/kathrynoconnor/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/Users/kathrynoconnor/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/Users/kathrynoconnor/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/Users/kathrynoconnor/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/Users/kathrynoconnor/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/Users/kathrynoconnor/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    }
   ],
   "source": [
    "xgb_auc_scores = cross_val_score(xgb_clf, \n",
    "                             X, \n",
    "                             y,\n",
    "                             scoring=\"roc_auc\", \n",
    "                             cv=10                               \n",
    ")\n",
    "xgb_mean_auc = np.mean(xgb_auc_scores)\n",
    "display_scores(xgb_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cm = confusion_matrix(y, xgb_y_pred)\n",
    "\n",
    "# Visualizing the confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for XGBoost Classifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca352a53",
   "metadata": {},
   "source": [
    "## XGBoost Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc95e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf_upsampled = XGBClassifier(use_label_encoder=False, \n",
    "                                  eval_metric='logloss', \n",
    "                                  n_estimators=100, \n",
    "                                  max_depth=6)\n",
    "\n",
    "# Train the model on the upsampled dataset\n",
    "xgb_clf_upsampled.fit(X_upsampled, y_upsampled)\n",
    "\n",
    "# Make predictions (as an example, using the same upsampled data for simplicity)\n",
    "y_pred_upsampled = xgb_clf_upsampled.predict(X_upsampled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_upsampled = accuracy_score(y_upsampled, y_pred_upsampled)\n",
    "print(\"Train Accuracy on Upsampled Data:\", accuracy_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_upsampled_cm = confusion_matrix(y, y_pred_upsampled)\n",
    "\n",
    "# Visualizing the confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for XGBoost Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb84b9d",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100, \n",
    "                                max_depth=4, \n",
    "                                random_state=42)\n",
    "\n",
    "rf_clf.fit(X, y)\n",
    "\n",
    "rf_y_pred = rf_clf.predict(X)\n",
    "\n",
    "accuracy = accuracy_score(y, rf_y_pred)\n",
    "print(\"Train Accuracy:\", accuracy)\n",
    "print(\"Train Accuracy using built-in score:\", rf_clf.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_auc_scores = cross_val_score(rf_clf, \n",
    "                             X, \n",
    "                             y,\n",
    "                             scoring=\"roc_auc\", \n",
    "                             cv=10                               \n",
    ")\n",
    "rf_mean_auc = np.mean(rf_auc_scores)\n",
    "display_scores(rf_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cm = confusion_matrix(y, rf_y_pred)\n",
    "\n",
    "# Visualizing the confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for Random Forest Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tree = rf_clf.estimators_[0]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(single_tree, filled=True, feature_names=X.columns, class_names=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a5611",
   "metadata": {},
   "source": [
    "## Random Forest Upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_upsampled = RandomForestClassifier(n_estimators=100, \n",
    "                                          max_depth=4, \n",
    "                                          random_state=42)\n",
    "\n",
    "rf_clf_upsampled.fit(X_upsampled, y_upsampled)\n",
    "\n",
    "# Making predictions on the upsampled training set for demonstration\n",
    "rf_y_pred_upsampled = rf_clf_upsampled.predict(X_upsampled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_upsampled = accuracy_score(y_upsampled, rf_y_pred_upsampled)\n",
    "print(\"Train Accuracy on Upsampled Data:\", accuracy_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88540ced",
   "metadata": {},
   "source": [
    "# Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fa784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assuming 'test_df' contains 'SK_ID_CURR' and you're about to predict\n",
    "# Save the IDs in a separate variable or ensure they're in 'test_df'\n",
    "ids = test_selected_df['SK_ID_CURR'].copy()\n",
    "\n",
    "# If you had to remove 'SK_ID_CURR' from test_df for prediction, do so\n",
    "test_features = test_selected_df.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "# Step 2: Predict TARGET on the test set\n",
    "xgb_y_test_pred = xgb_clf_upsampled.predict(test_features)\n",
    "\n",
    "# Step 3: Combine IDs with their corresponding predictions\n",
    "predictions_with_ids = pd.DataFrame({'SK_ID_CURR': ids, 'TARGET': xgb_y_test_pred})\n",
    "\n",
    "print(predictions_with_ids['TARGET'].value_counts())\n",
    "\n",
    "# Now you can see the dataset with IDs and the new TARGET column\n",
    "print(predictions_with_ids.head())\n",
    "sample = pd.read_csv(sample_sub)\n",
    "print(sample.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:\\\\Users\\\\Owner\\\\OneDrive\\\\Documents'\n",
    "filename = 'predictions_with_ids.csv'\n",
    "path = os.path.join(directory, filename)\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Now it's safe to write the file\n",
    "predictions_with_ids.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc95b4e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "During our exploratory data analysis (EDA) and initial model building, we identified two key challenges: the imbalanced distribution of the target variable and the presence of a substantial number of missing values. Addressing these challenges was crucial for enhancing our model performance.\n",
    "\n",
    "Upsampling emerged as a pivotal technique, significantly elevating the Area Under the Curve (AUC) scores across all our models. This outcome was particularly notable due to the imbalance in the target variable, with over 91% of outcomes being 0. By incorporating upsampling, our models were trained on a more balanced dataset, allowing them to capture the minority class (1) more effectively.\n",
    "\n",
    "The application of upsampling directly aligns with Home Credit's strategic objective of minimizing loan rejections for creditworthy individuals. We believe that leveraging techniques like upsampling in predictive modeling is instrumental in achieving this goal.\n",
    "\n",
    "Furthermore, we recommend exploring additional avenues such as interaction variables and feature engineering to further optimize predictive performance. These strategies can potentially unlock deeper insights from the data and enhance the robustness of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f525e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "342px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
